peft_config=LoraConfig(
    r=8,
    lora_alpha=16, 
    lora_dropout=0.2,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    bias="none",
    task_type="CAUSAL_LM"
)

num_train_epochs=1
per_device_train_batch_size=8
per_device_eval_batch_size=8
gradient_accumulation_steps=4
logging_steps=50
learning_rate=1e-4

lr_scheduler_type="linear"
